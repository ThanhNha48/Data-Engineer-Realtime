# ğŸš€ Real-Time Data Engineer Project

## ğŸ“Œ Overview

The **Real-Time Data Engineer Project** simulates a complete **real-time data processing pipeline**, using popular technologies commonly applied in modern Data Engineering systems:

* **Apache Airflow** â€“ Workflow orchestration & scheduling
* **Apache Kafka** â€“ Message queue / streaming platform
* **Apache Spark Structured Streaming** â€“ Real-time data processing
* **PostgreSQL** â€“ Data sink / warehouse
* **Docker & Docker Compose** â€“ Containerized environment

The pipeline **collects user data from an external API**, **streams it through Kafka**, **cleans and processes it using Spark**, and **stores the final results in PostgreSQL**.

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RandomUser â”‚
â”‚    API     â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
      â”‚ (REST API)
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Airflow   â”‚  (DAG: user_automation)
â”‚ (Python)   â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
      â”‚ (Kafka Producer)
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Kafka    â”‚  (Topic: users_created)
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
      â”‚ (Structured Streaming)
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Spark    â”‚  (Clean & Validate)
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
      â”‚ (JDBC)
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PostgreSQL â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ Detailed Data Flow

### 1ï¸âƒ£ Airflow â€“ Data Ingestion & Formatting

* DAG name: `user_automation`
* Schedule: `@daily`

**Task 1 â€“ get_and_format_data**

* Calls external API: `https://randomuser.me/api/`
* Formats user information (name, email, phone, address, etc.)
* Pushes formatted data to **XCom**

**Task 2 â€“ stream_to_kafka**

* Pulls data from XCom
* Streams JSON messages to Kafka topic `users_created`
* Streams continuously for **60 seconds**

---

### 2ï¸âƒ£ Kafka â€“ Message Streaming Layer

* Broker: `broker:29092`
* Topic: `users_created`
* Message format: JSON (UTF-8 encoded)

Kafka acts as a **decoupling layer** between data producers (Airflow) and consumers (Spark).

---

### 3ï¸âƒ£ Spark Structured Streaming â€“ Real-Time Processing

**Processing steps:**

* Read streaming data from Kafka (`readStream`)
* Parse JSON messages using `from_json` with a predefined schema
* Clean and standardize data:

  * Trim whitespace
  * Normalize uppercase/lowercase text
  * Validate email format
  * Normalize phone numbers
  * Convert timestamps
* Deduplicate records based on `email`

**Trigger interval:**

* `processingTime = 10 seconds`

---

### 4ï¸âƒ£ PostgreSQL â€“ Data Storage

* Database: `airflow`
* Table: `created_users`
* Data written using `foreachBatch` with JDBC
* Write mode: `append`

PostgreSQL serves as the **final data sink / warehouse layer**.

---

## ğŸ“‚ Project Structure (Recommended)

```
Data_Engineer_RealTime/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ user_automation.py      # Airflow DAG
â”œâ”€â”€ spark/
â”‚   â””â”€â”€ spark_streaming.py      # Spark Structured Streaming job
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ scripts/
```

---

## ğŸ§ª Data Quality & Validation

* Deduplicate records based on email
* Standardize and clean input data

---

## ğŸš€ How to Run the Project (Quick Start)

```bash
# Start all services
docker-compose up -d

# Access Airflow UI
http://localhost:8080

# Trigger the DAG: user_automation

ğŸ”— Access Services
Airflow UI: http://localhost:8080
Kafka Cluster: http://localhost:9021/Clusters
Spark UI: http://localhost:8083

## ğŸ”„ Active Streaming

## Test Image
![test](images/anh.jpg)
